{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8d3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6ddca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9001612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6127a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airport_inside', 'artstudio', 'auditorium', 'bakery', 'bar', 'bathroom', 'bedroom', 'bookstore', 'bowling', 'buffet', 'casino', 'children_room', 'church_inside', 'classroom', 'cloister', 'closet', 'clothingstore', 'computerroom', 'concert_hall', 'corridor', 'deli', 'dentaloffice', 'dining_room', 'elevator', 'fastfood_restaurant', 'florist', 'gameroom', 'garage', 'greenhouse', 'grocerystore', 'gym', 'hairsalon', 'hospitalroom', 'inside_bus', 'inside_subway', 'jewelleryshop', 'kindergarden', 'kitchen', 'laboratorywet', 'laundromat', 'library', 'livingroom', 'lobby', 'locker_room', 'mall', 'meeting_room', 'movietheater', 'museum', 'nursery', 'office', 'operating_room', 'pantry', 'poolinside', 'prisoncell', 'restaurant', 'restaurant_kitchen', 'shoeshop', 'stairscase', 'studiomusic', 'subway', 'toystore', 'trainstation', 'tv_studio', 'videostore', 'waitingroom', 'warehouse', 'winecellar']\n"
     ]
    }
   ],
   "source": [
    "data_dir  = 'c:/Users/Reliance Digital/Downloads/Images'\n",
    "\n",
    "classes = os.listdir(data_dir)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5389f939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000c0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_interior_classes = [\n",
    "    'livingroom', 'bedroom', 'kitchen', 'bathroom', 'dining_room', \n",
    "    'closet', 'pantry', 'children_room', 'nursery', 'home_office'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621fbafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c52074b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dir = './home_interior_dataset'\n",
    "os.makedirs(filtered_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3001ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subdirectories for each home interior class\n",
    "for class_name in home_interior_classes:\n",
    "    src_dir = os.path.join(data_dir, class_name)\n",
    "    dst_dir = os.path.join(filtered_dir, class_name)\n",
    "    if os.path.exists(src_dir):\n",
    "        shutil.copytree(src_dir, dst_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58308c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "241e539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered dataset\n",
    "dataset = ImageFolder(filtered_dir, transform=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f7177d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to show sample\n",
    "def show_sample(img, label):\n",
    "    print(\"Label:\", dataset.classes[label], \"(Class No: \"+ str(label) + \")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88cd0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ebb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3100049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class IndoorSceneClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=len(home_interior_classes)):\n",
    "        super(IndoorSceneClassifier, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51cf94e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Reliance Digital/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 55.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = IndoorSceneClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c01f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba8a14e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.5022 Acc: 0.0881\n"
     ]
    }
   ],
   "source": [
    "        # Validation\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "val_corrects = 0\n",
    "        \n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    val_acc = val_corrects.double() / len(val_dataset)\n",
    "        \n",
    "    print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac3729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.0942 Acc: 0.6303\n",
      "Validation Loss: 1.4639 Acc: 0.5373\n",
      "Epoch 2/10 - Loss: 0.6805 Acc: 0.7651\n",
      "Validation Loss: 2.1709 Acc: 0.4179\n",
      "Epoch 3/10 - Loss: 0.4319 Acc: 0.8514\n",
      "Validation Loss: 1.1955 Acc: 0.6000\n",
      "Epoch 4/10 - Loss: 0.3813 Acc: 0.8708\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define home interior classes\n",
    "home_interior_classes = [\n",
    "    'livingroom', 'bedroom', 'kitchen', 'bathroom', 'dining_room', \n",
    "    'closet', 'pantry', 'children_room', 'nursery', 'home_office'\n",
    "]\n",
    "\n",
    "# Original data directory\n",
    "data_dir = 'c:/Users/Reliance Digital/Downloads/Images'\n",
    "\n",
    "\n",
    "# Transformations\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load filtered dataset\n",
    "dataset = ImageFolder(filtered_dir, transform=transformations)\n",
    "\n",
    "# Helper function to show sample\n",
    "def show_sample(img, label):\n",
    "    print(\"Label:\", dataset.classes[label], \"(Class No: \"+ str(label) + \")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model definition\n",
    "class IndoorSceneClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=len(home_interior_classes)):\n",
    "        super(IndoorSceneClassifier, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = IndoorSceneClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "        val_loss = val_loss / len(val_dataset)\n",
    "        val_acc = val_corrects.double() / len(val_dataset)\n",
    "        \n",
    "        print(f'Validation Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'home_interior_classifier.pth')\n",
    "\n",
    "# Function to predict on new images\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = transformations(image).unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        \n",
    "    return dataset.classes[predicted.item()]\n",
    "\n",
    "# Example usage\n",
    "# predicted_class = predict_image('path_to_your_image.jpg')\n",
    "# print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0178ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'home_interior_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee28d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaff700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5624e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896be695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Reliance Digital\\AppData\\Local\\Temp\\ipykernel_8032\\1938595482.py\", line 5, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     ArrowDtype,\n\u001b[0;32m     52\u001b[0m     Int8Dtype,\n\u001b[0;32m     53\u001b[0m     Int16Dtype,\n\u001b[0;32m     54\u001b[0m     Int32Dtype,\n\u001b[0;32m     55\u001b[0m     Int64Dtype,\n\u001b[0;32m     56\u001b[0m     UInt8Dtype,\n\u001b[0;32m     57\u001b[0m     UInt16Dtype,\n\u001b[0;32m     58\u001b[0m     UInt32Dtype,\n\u001b[0;32m     59\u001b[0m     UInt64Dtype,\n\u001b[0;32m     60\u001b[0m     Float32Dtype,\n\u001b[0;32m     61\u001b[0m     Float64Dtype,\n\u001b[0;32m     62\u001b[0m     CategoricalDtype,\n\u001b[0;32m     63\u001b[0m     PeriodDtype,\n\u001b[0;32m     64\u001b[0m     IntervalDtype,\n\u001b[0;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     66\u001b[0m     StringDtype,\n\u001b[0;32m     67\u001b[0m     BooleanDtype,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     NA,\n\u001b[0;32m     70\u001b[0m     isna,\n\u001b[0;32m     71\u001b[0m     isnull,\n\u001b[0;32m     72\u001b[0m     notna,\n\u001b[0;32m     73\u001b[0m     notnull,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Index,\n\u001b[0;32m     76\u001b[0m     CategoricalIndex,\n\u001b[0;32m     77\u001b[0m     RangeIndex,\n\u001b[0;32m     78\u001b[0m     MultiIndex,\n\u001b[0;32m     79\u001b[0m     IntervalIndex,\n\u001b[0;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     81\u001b[0m     DatetimeIndex,\n\u001b[0;32m     82\u001b[0m     PeriodIndex,\n\u001b[0;32m     83\u001b[0m     IndexSlice,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     NaT,\n\u001b[0;32m     86\u001b[0m     Period,\n\u001b[0;32m     87\u001b[0m     period_range,\n\u001b[0;32m     88\u001b[0m     Timedelta,\n\u001b[0;32m     89\u001b[0m     timedelta_range,\n\u001b[0;32m     90\u001b[0m     Timestamp,\n\u001b[0;32m     91\u001b[0m     date_range,\n\u001b[0;32m     92\u001b[0m     bdate_range,\n\u001b[0;32m     93\u001b[0m     Interval,\n\u001b[0;32m     94\u001b[0m     interval_range,\n\u001b[0;32m     95\u001b[0m     DateOffset,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     to_numeric,\n\u001b[0;32m     98\u001b[0m     to_datetime,\n\u001b[0;32m     99\u001b[0m     to_timedelta,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     Flags,\n\u001b[0;32m    102\u001b[0m     Grouper,\n\u001b[0;32m    103\u001b[0m     factorize,\n\u001b[0;32m    104\u001b[0m     unique,\n\u001b[0;32m    105\u001b[0m     value_counts,\n\u001b[0;32m    106\u001b[0m     NamedAgg,\n\u001b[0;32m    107\u001b[0m     array,\n\u001b[0;32m    108\u001b[0m     Categorical,\n\u001b[0;32m    109\u001b[0m     set_eng_float_format,\n\u001b[0;32m    110\u001b[0m     Series,\n\u001b[0;32m    111\u001b[0m     DataFrame,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.ops import box_iou\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =============================================\n",
    "# Configuration\n",
    "# =============================================\n",
    "class Config:\n",
    "    # Dataset\n",
    "    data_dir = \"home_interior_dataset\"  # Update this\n",
    "    classes = [\n",
    "        'livingroom', 'bedroom', 'kitchen', 'bathroom', 'dining_room',\n",
    "        'closet', 'pantry', 'children_room', 'nursery'\n",
    "    ]\n",
    "    furniture_classes = [\n",
    "        '__background__', 'bed', 'chair', 'sofa', 'table', 'desk',\n",
    "        'cabinet', 'shelf', 'refrigerator', 'oven', 'toilet'\n",
    "    ]\n",
    "    \n",
    "    # Image parameters\n",
    "    cls_img_size = 384\n",
    "    det_img_size = 512\n",
    "    batch_size = 16\n",
    "    \n",
    "    # Training\n",
    "    num_epochs = 30\n",
    "    lr = 1e-4\n",
    "    weight_decay = 1e-4\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# =============================================\n",
    "# ETL Pipeline\n",
    "# =============================================\n",
    "def create_annotation_structure():\n",
    "    \"\"\"Generate directory structure and annotation template\"\"\"\n",
    "    os.makedirs(os.path.join(config.data_dir, \"annotations\"), exist_ok=True)\n",
    "    \n",
    "    structure = {}\n",
    "    for cls in config.classes:\n",
    "        cls_dir = os.path.join(config.data_dir, cls)\n",
    "        if os.path.exists(cls_dir):\n",
    "            images = [f for f in os.listdir(cls_dir) if f.endswith(('.jpg', '.png'))]\n",
    "            for img in images:\n",
    "                img_id = f\"{cls}_{img}\"\n",
    "                img_path = os.path.join(cls_dir, img)\n",
    "                with Image.open(img_path) as im:\n",
    "                    width, height = im.size\n",
    "                \n",
    "                structure[img_id] = {\n",
    "                    \"file_path\": img_path,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height,\n",
    "                    \"class\": cls,\n",
    "                    \"boxes\": [],\n",
    "                    \"labels\": []\n",
    "                }\n",
    "    \n",
    "    # Save template\n",
    "    with open(os.path.join(config.data_dir, \"annotations/template.json\"), 'w') as f:\n",
    "        json.dump(structure, f)\n",
    "    print(\"Annotation template created. Please manually annotate or use synthetic data.\")\n",
    "\n",
    "# =============================================\n",
    "# EDA\n",
    "# =============================================\n",
    "def perform_eda():\n",
    "    \"\"\"Exploratory Data Analysis\"\"\"\n",
    "    class_counts = {}\n",
    "    image_sizes = []\n",
    "    \n",
    "    for cls in config.classes:\n",
    "        cls_dir = os.path.join(config.data_dir, cls)\n",
    "        if os.path.exists(cls_dir):\n",
    "            images = os.listdir(cls_dir)\n",
    "            class_counts[cls] = len(images)\n",
    "            \n",
    "            # Sample image size\n",
    "            img = random.choice(images)\n",
    "            with Image.open(os.path.join(cls_dir, img)) as im:\n",
    "                image_sizes.append(im.size)\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot image sizes\n",
    "    plt.subplot(1, 2, 2)\n",
    "    widths, heights = zip(*image_sizes)\n",
    "    sns.scatterplot(x=widths, y=heights, alpha=0.5)\n",
    "    plt.title('Image Size Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total classes: {len(class_counts)}\")\n",
    "    print(f\"Total images: {sum(class_counts.values())}\")\n",
    "    print(f\"Average per class: {sum(class_counts.values())/len(class_counts):.1f}\\n\")\n",
    "    print(\"Class counts:\", class_counts)\n",
    "\n",
    "# =============================================\n",
    "# Data Preprocessing\n",
    "# =============================================\n",
    "class RoomDataset(Dataset):\n",
    "    \"\"\"Classification Dataset\"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(config.classes)}\n",
    "        self.images = self._load_images()\n",
    "        \n",
    "    def _load_images(self):\n",
    "        images = []\n",
    "        for cls in config.classes:\n",
    "            cls_dir = os.path.join(self.root, cls)\n",
    "            if os.path.exists(cls_dir):\n",
    "                for img in os.listdir(cls_dir):\n",
    "                    if img.endswith(('.jpg', '.png')):\n",
    "                        images.append((os.path.join(cls_dir, img), self.class_to_idx[cls]))\n",
    "        return images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label\n",
    "\n",
    "def get_classification_transforms():\n",
    "    \"\"\"Data augmentation for classification\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((config.cls_img_size, config.cls_img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((config.cls_img_size, config.cls_img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "# =============================================\n",
    "# Model Training (Classification)\n",
    "# =============================================\n",
    "def train_classification_model():\n",
    "    \"\"\"Train room classifier\"\"\"\n",
    "    train_transform, val_transform = get_classification_transforms()\n",
    "    \n",
    "    # Load datasets\n",
    "    full_dataset = RoomDataset(config.data_dir, transform=train_transform)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(full_dataset))),\n",
    "        test_size=0.2,\n",
    "        stratify=[label for _, label in full_dataset.images]\n",
    "    )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.batch_size, shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=config.batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = models.efficientnet_b3(pretrained=True)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(config.classes))\n",
    "    model = model.to(config.device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.lr,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(config.device), labels.to(config.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(config.device), labels.to(config.device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct.double() / len(val_dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "        print(f\"Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_classifier.pth\")\n",
    "    \n",
    "    print(f\"Best Val Acc: {best_acc:.4f}\")\n",
    "\n",
    "# =============================================\n",
    "# Object Detection (YOLOv8)\n",
    "# =============================================\n",
    "def prepare_detection_data():\n",
    "    \"\"\"Convert annotations to YOLO format\"\"\"\n",
    "    os.makedirs(\"yolo_data\", exist_ok=True)\n",
    "    os.makedirs(\"yolo_data/images/train\", exist_ok=True)\n",
    "    os.makedirs(\"yolo_data/labels/train\", exist_ok=True)\n",
    "    \n",
    "    # Load annotations (replace with your annotation file)\n",
    "    with open(os.path.join(config.data_dir, \"annotations.json\")) as f:\n",
    "        annotations = json.load(f)\n",
    "    \n",
    "    for img_id, ann in annotations.items():\n",
    "        # Copy image\n",
    "        img = Image.open(ann[\"file_path\"])\n",
    "        img.save(f\"yolo_data/images/train/{img_id}.jpg\")\n",
    "        \n",
    "        # Create YOLO format label file\n",
    "        label_lines = []\n",
    "        for box, label_idx in zip(ann[\"boxes\"], ann[\"labels\"]):\n",
    "            x1, y1, x2, y2 = box\n",
    "            width = ann[\"width\"]\n",
    "            height = ann[\"height\"]\n",
    "            \n",
    "            # Convert to YOLO format (normalized cx, cy, w, h)\n",
    "            cx = ((x1 + x2) / 2) / width\n",
    "            cy = ((y1 + y2) / 2) / height\n",
    "            w = (x2 - x1) / width\n",
    "            h = (y2 - y1) / height\n",
    "            \n",
    "            label_lines.append(f\"{label_idx} {cx} {cy} {w} {h}\\n\")\n",
    "        \n",
    "        with open(f\"yolo_data/labels/train/{img_id}.txt\", 'w') as f:\n",
    "            f.writelines(label_lines)\n",
    "    \n",
    "    print(\"YOLO dataset prepared\")\n",
    "\n",
    "def train_detection_model():\n",
    "    \"\"\"Train YOLOv8 detector\"\"\"\n",
    "    # Create YOLO config file\n",
    "    yolo_config = {\n",
    "        \"path\": \"yolo_data\",\n",
    "        \"train\": \"images/train\",\n",
    "        \"val\": \"images/train\",  # For demo, use same as train\n",
    "        \"names\": {i: name for i, name in enumerate(config.furniture_classes)}\n",
    "    }\n",
    "    \n",
    "    with open(\"yolo_data/furniture.yaml\", 'w') as f:\n",
    "        yaml.dump(yolo_config, f)\n",
    "    \n",
    "    # Load and train model\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    model.train(\n",
    "        data=\"yolo_data/furniture.yaml\",\n",
    "        epochs=50,\n",
    "        imgsz=config.det_img_size,\n",
    "        batch=config.batch_size\n",
    "    )\n",
    "    \n",
    "    # Save best model\n",
    "    model.export(format=\"onnx\")\n",
    "\n",
    "# =============================================\n",
    "# Main Execution\n",
    "# =============================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: EDA\n",
    "    print(\"Performing EDA...\")\n",
    "    perform_eda()\n",
    "    \n",
    "    # Step 2: Classification Training\n",
    "    print(\"\\nTraining Classification Model...\")\n",
    "    train_classification_model()\n",
    "    \n",
    "    # Step 3: Detection Training\n",
    "    print(\"\\nPreparing Detection Data...\")\n",
    "    prepare_detection_data()\n",
    "    print(\"Training Detection Model...\")\n",
    "    train_detection_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "dataset = ImageFolder(data_dir, transform = transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e07c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_sample(img, label):\n",
    "    print(\"Label:\", dataset.classes[label], \"(Class No: \"+ str(label) + \")\")\n",
    "    plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = dataset[50]\n",
    "show_sample(img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af076a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5791e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = random_split(dataset, [13000, 2000, 620])\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026535f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(val_ds, batch_size*2, num_workers = 4, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894286d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images, nrow = 16).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d733af",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c847a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c5335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
